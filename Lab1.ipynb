{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Lab overview:\n",
        "\n",
        "\n",
        "* Normalization\n",
        "* Tokenization\n",
        "* Lematization\n",
        "* Stemming\n",
        "* Stopwords removal\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nK8jcjSNkv3"
      },
      "source": [
        "##Text normalization (cleaning)\n",
        "\n",
        "Depending on the task you are cleaning the text for, you may perform one or more of: \n",
        "\n",
        "* Transform text to lowercase\n",
        "* Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "* Remove punctuation\n",
        "* Remove digits or transform them to words\n",
        "* Correct spelling errors\n",
        "\n",
        "\n",
        "Python Regular Expressions \n",
        "*   [re Python documentation](https://docs.python.org/3/library/re.html)\n",
        "*   [Quick reference](https://www.computerhope.com/unix/regex-quickref.htm)\n",
        "*   [Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-zwIaLJ_gI"
      },
      "source": [
        "Transform text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mwbmESJGWAre",
        "outputId": "8614c70b-d685-4fdf-dbf0-fae82489dc01"
      },
      "source": [
        "import re\n",
        "\n",
        "text = ''' \" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you \n",
        "        a single image on screen. \\nThe movie opens with blackness, and only distant, \n",
        "        alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'''\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you \\n        a single image on screen. \\nthe movie opens with blackness, and only distant, \\n        alien-like underwater sounds. :) :d it deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ioMoqMP4sW"
      },
      "source": [
        "Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1PM0guG_P60M",
        "outputId": "685aedba-be26-441a-f85c-e7ac07d2c304"
      },
      "source": [
        "re.sub(' \\d+', '', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you \\n        a single image on screen. \\nthe movie opens with blackness, and only distant, \\n        alien-like underwater sounds. :) :d it deserves stars, not stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQEFmcfRPYc"
      },
      "source": [
        "Converting numbers to words using [num2words](https://github.com/savoirfairelinux/num2words) (it works on multiple languages)\n",
        "\n",
        "We need to install the num2words library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vx3KhgnRPfv",
        "outputId": "793feccd-70ca-4588-ef6f-9324cfd2ac8b"
      },
      "source": [
        "!pip install num2words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñé                            | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 30 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 40 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 51 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 61 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 71 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 81 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 92 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1K45N29RlOX"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9Egr4m_9Rl9b",
        "outputId": "09254194-2c47-4f73-cb30-b4ba39ee747e"
      },
      "source": [
        "from num2words import num2words\n",
        "\n",
        "text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
        "text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0HdtPGr68yQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEeAr6pKFj0"
      },
      "source": [
        "Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "\n",
        "Using [emoji](https://github.com/carpedm20/emoji) library or the corresponding unicode characters.\n",
        "\n",
        "We need to install the emoji library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq72wdvEKI7z",
        "outputId": "4adb2a8b-319a-4f82-fc6d-c77188100b0b"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.0.tar.gz (168 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà                              | 10 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 20 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 30 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 40 kB 40.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 51 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 61 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 71 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 92 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 102 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 112 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 122 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 133 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 143 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 153 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 163 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168 kB 16.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.0-py3-none-any.whl size=168256 sha256=fc2980c7024f838df62ca4b4baba129d6fee62642ae4dee2d3347dfced8ea721\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/d7/74/c720aaf345a042b0c2d74361873258c5e8649b7f11b2ccce49\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVu11QZEKoSW"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4HYmbrutKnEo",
        "outputId": "69d2d465-f6ec-438b-f582-1cbd0f4c9966"
      },
      "source": [
        "import emoji\n",
        "\n",
        "emoji.get_emoji_regexp().sub(u'', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. \\nthe movie opens with blackness, and only distant, alien-like underwater sounds. :) :d'"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69dNffKLtqu"
      },
      "source": [
        "The *get_emoji_regexp()* function returns a regex to match any emoji.\n",
        "\n",
        "Another way of removing emojis with regex:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "vHSh8NUIM_bY",
        "outputId": "f4e08edf-8b8c-4046-aeda-ee8ef148b4db"
      },
      "source": [
        "emoj = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2640-\\u2642\" \n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"\n",
        "    u\"\\u3030\"\n",
        "    \"]+\", re.UNICODE)\n",
        "\n",
        "text = re.sub(emoj, '', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds.   it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_xacsPCOPjL"
      },
      "source": [
        "Removing emoticons (regex from [nltk Twitter Tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "QeYkCRPNOS3n",
        "outputId": "04b2f21c-79af-429f-9bec-05837c902476"
      },
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      </?3                       # heart\n",
        "    )\"\"\"\n",
        "    \n",
        "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "text = re.sub(emoticon_re, '', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds.   it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VkbsLqXMyYo"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "*   Word level: Split by whitespace, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "*   Sentence level: Split by punctuation, [nltk.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrN047ULHZQ_",
        "outputId": "e9ada44c-df6d-46c6-b421-bd0b608f73dd"
      },
      "source": [
        "print(text.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', 'jaws', '\"', 'ü¶àü¶àü¶à', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen.', 'the', 'movie', 'opens', 'with', 'blackness,', 'and', 'only', 'distant,', 'alien-like', 'underwater', 'sounds.', ':)', ':d', 'it', 'deserves', '5', 'stars,', 'not', '4', 'stars.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyTVP55VRwT"
      },
      "source": [
        "We need to download first the Punkt Tokenizer Models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHvjfBzxVdAv",
        "outputId": "0a5ad69b-d65a-4d29-eace-ada6c7f3152e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nZasrqFVE-h",
        "outputId": "aa6f444d-0f25-41e7-a1b8-d057fd55e523"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "tokenized_text_nltk = word_tokenize(text)\n",
        "print(tokenized_text_nltk)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jaws', 'ü¶àü¶àü¶à', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen', 'the', 'movie', 'opens', 'with', 'blackness', 'and', 'only', 'distant', 'alienlike', 'underwater', 'sounds', 'd', 'it', 'deserves', '5', 'stars', 'not', '4', 'stars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yypjBlMVpnG"
      },
      "source": [
        "Sentence tokenization using regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqqArIwwVp1y",
        "outputId": "9b665dff-dacb-4da0-b39c-09ae6927aa4f"
      },
      "source": [
        " re.split('(?<=[.!?]) +', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7KKoyA1WNVo"
      },
      "source": [
        "Sentence tokenization using nltk.sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RfpgBzWSaL",
        "outputId": "8de74d3a-108e-46a3-a5ff-db6feb511a58"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM9idFgSWnJp",
        "outputId": "8fd9bda7-7194-4cfa-a8a0-c163b02db94d"
      },
      "source": [
        "text_example = 'I was good.Thanks.'\n",
        "re.split('(?<=[.!?]) +', text_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0DKB0VoWr8M",
        "outputId": "1d6d5ab0-debb-49a1-8865-136f32d1c024"
      },
      "source": [
        "nltk.sent_tokenize(text_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s4GXfHXCOl"
      },
      "source": [
        "Removing punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zCxHclC0XF8e",
        "outputId": "28b46568-93e7-4af0-a8ea-7e6b1ec80938"
      },
      "source": [
        "re.sub(r'[^\\w\\s]','', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  jaws   is a rare film that grabs your attention before it shows you \\n a single image on screen \\nthe movie opens with blackness and only distant \\n alienlike underwater sounds  d it deserves 5 stars not 4 stars'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Jid14LagYAPQ",
        "outputId": "462db9f1-542d-4d8e-f4d3-629bfe6411a0"
      },
      "source": [
        "import string\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  jaws  ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you \\n        a single image on screen \\nthe movie opens with blackness and only distant \\n        alienlike underwater sounds  d it deserves 5 stars not 4 stars'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkRYWmakLla"
      },
      "source": [
        "Removing multiple spaces between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LHAFnlSzYu7_",
        "outputId": "93557037-87ef-49e7-ff69-3cd246d12f94"
      },
      "source": [
        "text = re.sub(' +', ' ', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you \\n a single image on screen. \\nthe movie opens with blackness, and only distant, \\n alien-like underwater sounds. :) :d it deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F0pEMhIHbd6"
      },
      "source": [
        "## Removing stopwords\n",
        "\n",
        "![stopwords.jpg](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png) \n",
        "\n",
        "[Photo credits](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980726-remove-stop-words-from-a-block-of-text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53_oLK7NP8y"
      },
      "source": [
        "###Why do we Need to Remove Stopwords?\n",
        "\n",
        "For tasks such as text classification, we may want to remove any unnecessary words and keep only words with meaning. \n",
        "\n",
        "Stopwords removal is not used in tasks such as machine translation or text summarization.\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGkYhh5oMcx1"
      },
      "source": [
        "Stopwords removal using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idPo-mzrC2HW",
        "outputId": "ec0ee360-9e5b-43e8-96f1-e73697b8371c"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "179\n",
            "{'all', 'doing', 'than', 'while', \"wouldn't\", 'further', 'each', 'yourself', 'does', 'or', 'did', 'mustn', 'that', 'as', 'out', 'shouldn', 'he', 'between', 'am', \"you'll\", 'was', \"it's\", 'been', 'up', \"wasn't\", 'myself', \"doesn't\", \"she's\", 'we', \"that'll\", 'those', 'if', 'more', 'have', 'before', \"shan't\", 'him', 'no', 'its', 'were', \"you've\", 'having', 'into', 'not', 'll', 'on', 'which', \"haven't\", 'then', 'wasn', 'can', 'y', 'over', 'a', 'there', \"mightn't\", 'his', 'theirs', \"needn't\", 'won', 'yourselves', \"weren't\", 'until', 'some', 'isn', 'because', 'are', 'just', 'why', 'only', 'm', 'being', 'too', 'where', 'had', 'down', 'hers', 'needn', 'in', 'below', 'with', 'didn', \"hasn't\", 'about', 'against', 'shan', 'off', 'yours', 'our', 'so', \"mustn't\", 'both', 'your', 'they', 'once', 'weren', 'above', 'doesn', 'ain', 'mightn', \"don't\", 'an', 'by', \"didn't\", 'the', 'under', 'themselves', 'whom', 'such', 'ma', 'them', \"won't\", 'their', 'but', 'couldn', 'should', 'itself', 'aren', 'for', 'me', 'who', \"couldn't\", 'herself', 'same', 'you', 'has', \"aren't\", 'my', 'again', \"isn't\", 'here', 're', 'hadn', \"shouldn't\", 'to', 'hasn', 'own', 'other', 'at', 'do', 'how', 'any', 'd', \"you'd\", 'very', 've', 's', 'this', 'after', 'when', 'wouldn', 'and', 'nor', 'it', 'from', 'few', \"should've\", 't', 'what', 'ours', 'is', 'during', 'will', 'be', \"you're\", 'through', 'her', \"hadn't\", 'don', 'these', 'she', 'o', 'of', 'most', 'haven', 'himself', 'ourselves', 'now', 'i'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY7q0410L7t0",
        "outputId": "87912466-b1d2-47e9-b5ee-8c8a04c5ba21"
      },
      "source": [
        "tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jaws', 'ü¶àü¶àü¶à', 'rare', 'film', 'grabs', 'attention', 'shows', 'single', 'image', 'screen', 'movie', 'opens', 'blackness', 'distant', 'alienlike', 'underwater', 'sounds', 'deserves', '5', 'stars', '4', 'stars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3FIzVTMYc7"
      },
      "source": [
        "Stopwords removal using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys1CTPNoMgNv",
        "outputId": "c44290e1-8656-4bca-f59d-279f9b82fdea"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "{'all', 'doing', 'toward', 'part', 'could', 'twenty', 'whenever', \"'re\", 'somewhere', 'nobody', 'beforehand', 'he', 'thereby', 'always', 'thereafter', '‚Äôre', 'whether', 'have', 'before', 'around', 'into', 'fifty', 'which', 'either', 'can', 'mostly', 'a', '‚Äòs', 'please', 'because', 'elsewhere', 'never', 'behind', 'one', 'still', 'must', 'unless', 'become', 'where', 'had', 'latter', 'name', 'hereafter', 'least', 'below', 'with', 'everyone', 'quite', 'off', 'made', 'whose', 'beside', 'show', 'namely', 'n‚Äòt', 'whence', 'the', 'afterwards', 'thru', 'thus', 'keep', 'amongst', 'else', 'any', 'very', 'herein', 'after', 'various', 'somehow', 'meanwhile', 'throughout', 'when', '‚Äòm', 'be', 'formerly', 'sometime', 'top', 'often', 'these', 'towards', 'used', 'besides', 'while', 'really', 'further', \"'ve\", 'or', 'did', 'everywhere', 'n‚Äôt', 'next', \"n't\", 'forty', 'we', 'anything', 'someone', 'everything', 'those', 'anyway', 'noone', 'not', 'indeed', 'on', 'although', '‚Äòve', 'amount', 'beyond', '‚Äòd', 'over', 'another', 'until', 'two', 'less', 'anyone', 'only', 'hers', 'mine', 'empty', 'see', 'us', '‚Äôs', '‚Äôll', 'about', 'against', 'yours', 'also', 'our', 'make', 'since', 'both', '‚Äôd', 'perhaps', 'whereas', 'once', 'wherever', \"'s\", 'whereby', 'give', 'under', 'five', 'such', 'well', 'their', 'already', 'take', 'my', 'hundred', 'per', 'own', 'third', 'four', 'other', 'along', 'sixty', 'anyhow', 'sometimes', 'together', 'from', 'few', 'what', 'is', 'none', 'her', 'twelve', 'of', 'himself', 'ourselves', 'side', 'within', 'even', 'say', 'seeming', 'each', 'yourself', 'yet', 'out', 'might', 'was', 'whatever', 'been', 'may', 'myself', 'almost', 'therefore', \"'m\", 'due', 'full', 'put', 'its', 'were', 'regarding', 'nevertheless', 'seem', 'back', 'yourselves', 'move', 'via', 'being', 'too', 'down', 'in', 'without', 'eleven', 'so', 'thence', 'whoever', 'thereupon', 'eight', 'above', 'using', 'serious', 'an', 'nothing', 'by', '‚Äòll', 'seemed', 'ever', 'them', 'seems', 'ten', 'moreover', 'for', 'anywhere', 'me', 'hence', 'herself', 'former', 'same', 'neither', 'again', 'here', 're', 'at', 'do', 'how', 'this', 'last', 'nor', 'becoming', 'enough', 'go', 'during', 'cannot', 'most', 'i', 'bottom', 'than', 'does', 'would', 'three', 'that', 'as', 'whither', \"'ll\", 'several', 'others', 'between', 'am', 'nine', 'up', 'hereby', 'if', 'more', 'him', 'no', 'then', 'became', 'there', 'his', 'first', 'onto', 'some', 'fifteen', 'ca', 'are', 'call', 'just', 'why', 'front', 'among', 'six', 'alone', 'whereupon', 'hereupon', 'latterly', 'across', 'rather', 'your', '‚Äôm', 'they', '‚Äòre', 'though', 'wherein', '‚Äôve', 'done', 'themselves', 'whom', 'however', 'whole', 'except', 'but', 'get', 'should', 'itself', 'much', 'who', 'has', 'you', 'something', 'to', 'becomes', 'many', 'and', 'it', 'otherwise', 'every', 'ours', 'upon', 'will', 'through', 'whereafter', 'she', \"'d\", 'therein', 'now', 'nowhere'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE9rct__Oi7a",
        "outputId": "a40dd1dc-f4d5-4400-f41e-9db31e4d23d6"
      },
      "source": [
        "tokenized_text_spacy = nlp(text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  , jaws,  , ü¶à, ü¶à, ü¶à, is, a, rare, film, that, grabs, your, attention, before, it, shows, you, \n",
            "        , a, single, image, on, screen, \n",
            ", the, movie, opens, with, blackness, and, only, distant, \n",
            "        , alienlike, underwater, sounds,  , d, it, deserves, 5, stars, not, 4, stars]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH15qHZ3HYga"
      },
      "source": [
        "## Lematization/Stemming\n",
        "\n",
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)\n",
        "\n",
        "[Photo credits](https://tr.pinterest.com/pin/706854104005417976/)\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOVeBOOYZo7F"
      },
      "source": [
        "Lematization\n",
        "\n",
        "Using the WordNetLemmatizer from nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E89_jM6NY_nh",
        "outputId": "497b3a9a-e267-4c2c-c45d-c3dc1b0813c8"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxu3jUM1UviR",
        "outputId": "d66f1aa1-e2c7-4416-a381-a87c894e3ef7"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = word_tokenize(text)\n",
        "for word in words:\n",
        "    print(word, lemmatizer.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaws jaw\n",
            "ü¶àü¶àü¶à ü¶àü¶àü¶à\n",
            "is is\n",
            "a a\n",
            "rare rare\n",
            "film film\n",
            "that that\n",
            "grabs grab\n",
            "your your\n",
            "attention attention\n",
            "before before\n",
            "it it\n",
            "shows show\n",
            "you you\n",
            "a a\n",
            "single single\n",
            "image image\n",
            "on on\n",
            "screen screen\n",
            "the the\n",
            "movie movie\n",
            "opens open\n",
            "with with\n",
            "blackness blackness\n",
            "and and\n",
            "only only\n",
            "distant distant\n",
            "alienlike alienlike\n",
            "underwater underwater\n",
            "sounds sound\n",
            "d d\n",
            "it it\n",
            "deserves deserves\n",
            "5 5\n",
            "stars star\n",
            "not not\n",
            "4 4\n",
            "stars star\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWPz08R0qD79"
      },
      "source": [
        "Using the [lemmatizer](https://spacy.io/api/lemmatizer) from spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5oA4G1muu4y",
        "outputId": "ea2ed7f7-9c0b-4d1a-a039-e1b99ac4d01d"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-58.2.0-py3-none-any.whl (946 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-21.3 setuptools-58.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (58.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6 MB 74 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11YnUMJpqEO3",
        "outputId": "a5d685b0-79b1-4e27-8e1b-e256cb4feaa5"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \n",
            "jaws jaw\n",
            "   \n",
            "ü¶à ü¶à\n",
            "ü¶à ü¶à\n",
            "ü¶à ü¶à\n",
            "is be\n",
            "a a\n",
            "rare rare\n",
            "film film\n",
            "that that\n",
            "grabs grab\n",
            "your your\n",
            "attention attention\n",
            "before before\n",
            "it it\n",
            "shows show\n",
            "you you\n",
            "\n",
            "         \n",
            "        \n",
            "a a\n",
            "single single\n",
            "image image\n",
            "on on\n",
            "screen screen\n",
            "\n",
            " \n",
            "\n",
            "the the\n",
            "movie movie\n",
            "opens open\n",
            "with with\n",
            "blackness blackness\n",
            "and and\n",
            "only only\n",
            "distant distant\n",
            "\n",
            "         \n",
            "        \n",
            "alienlike alienlike\n",
            "underwater underwater\n",
            "sounds sound\n",
            "   \n",
            "d d\n",
            "it it\n",
            "deserves deserve\n",
            "5 5\n",
            "stars star\n",
            "not not\n",
            "4 4\n",
            "stars star\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMUayDqZweHR"
      },
      "source": [
        "Stemming in using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7dDNx1weU3",
        "outputId": "4c788f74-149d-4be1-d47b-89e1e69c90a7"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word, ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaws jaw\n",
            "ü¶àü¶àü¶à ü¶àü¶àü¶à\n",
            "is is\n",
            "a a\n",
            "rare rare\n",
            "film film\n",
            "that that\n",
            "grabs grab\n",
            "your your\n",
            "attention attent\n",
            "before befor\n",
            "it it\n",
            "shows show\n",
            "you you\n",
            "a a\n",
            "single singl\n",
            "image imag\n",
            "on on\n",
            "screen screen\n",
            "the the\n",
            "movie movi\n",
            "opens open\n",
            "with with\n",
            "blackness black\n",
            "and and\n",
            "only onli\n",
            "distant distant\n",
            "alienlike alienlik\n",
            "underwater underwat\n",
            "sounds sound\n",
            "d d\n",
            "it it\n",
            "deserves deserv\n",
            "5 5\n",
            "stars star\n",
            "not not\n",
            "4 4\n",
            "stars star\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmgsTVhIZuS9"
      },
      "source": [
        "[Other stemmers in nltk](https://www.nltk.org/api/nltk.stem.html)\n",
        "\n",
        "The spacy library does not perform stemming, only lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/ygCNwFM4i5RMPtsC6\n",
        "\n",
        "Preprocess texts from Twitter\n",
        "\n",
        "## Data\n",
        "\n",
        "We will use the twitter corpus from nltk, usually used in sentiment analysis.\n",
        "\n",
        "The fist step is downloading the dataset using the *download* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fLAUv1MRdHq",
        "outputId": "6bab773f-3c74-4142-93a0-e00beb1a4814"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_VdfEmjKXh"
      },
      "source": [
        "In order to inspect our data, we look at the first 25 tweets from the dataset. The text contains a lot of mentions, hashtags and emoticons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIODYYSvSuWj",
        "outputId": "3a50b6af-28e0-4c30-ce35-1c07746c4aca"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "tweets[:25]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
              " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
              " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
              " '@Impatientraider On second thought, there‚Äôs just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
              " 'Jgh , but we have to go to Bayan :D bye',\n",
              " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell‚Ä¶ as the name implies :p.',\n",
              " '#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0',\n",
              " 'Laying out a greetings card range for print today - love my job :-)',\n",
              " \"Friend's lunch... yummmm :)\\n#Nostalgia #TBS #KU.\",\n",
              " \"@RookieSenpai @arcadester it is the id conflict thanks for the help :D here's the screenshot of it working\",\n",
              " '@oohdawg_ Hi liv :))',\n",
              " 'Hello I need to know something can u fm me on Twitter?? ‚Äî sure thing :) dm me x http://t.co/W6Dy130BV7',\n",
              " '#FollowFriday @MBandScott_ @Eric_FLE @pointsolutions3 for being top new followers in my community this week :)',\n",
              " \"@rossbreadmore I've heard the Four Seasons is pretty dope. Penthouse, obvs #Gobigorgohome\\nHave fun y'all :)\",\n",
              " '@gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))',\n",
              " 'Hello :) Get Youth Job Opportunities follow &gt;&gt; @tolajobjobs @maphisa301',\n",
              " \"üíÖüèΩüíã - :)))) haven't seen you in years\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V_Y5pHtc07i"
      },
      "source": [
        "**Given a list of tweets, preprocess each tweet from the list.**\n",
        "\n",
        "**Instructions**: Implement the *preprocess* function. You can do the text cleaning in any order you prefer.\n",
        "\n",
        "**Hint**: You may need to use regex expressions (use the resources provided above).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAmrMgS7QA"
      },
      "source": [
        "def preprocess(tweets):\n",
        "\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        tweets: a list of tweets\n",
        "    Output: \n",
        "        prepocessed_tweets: a list of preprocessed tweets\n",
        "    \"\"\"\n",
        "\n",
        "    preprocessed_tweets = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "\n",
        "        ###remove new line characters '\\n'\n",
        "        ###remove links http://t.co/of3DyOzML0\n",
        "        ###remove mentions '@'\n",
        "        ###remove hashtags '#'\n",
        "        ###lowercase text\n",
        "        ###remove emojis 'üëå üç≠ :) :D'\n",
        "        ###remove digits\n",
        "        ###remove punctuation\n",
        "        ###tokenize tweet into separate words\n",
        "        ###lematization or stemming\n",
        "        ###remove stopwords\n",
        "        \n",
        "        preprocessed_tweets.append(words)\n",
        "    \n",
        "    return prepocessed_tweets\n",
        "\n",
        "preprocess(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDYpBHBOqTK7"
      },
      "source": [
        "Using NLTK‚Äôs Pre-Trained Sentiment Analyzer **VADER** (Valence Aware Dictionary and sEntiment Reasoner)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpe4sC1Qqk1r",
        "outputId": "cca87723-f285-46ef-a919-14a9afb935fa"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpr03xAMqK25",
        "outputId": "090741fa-a518-4e7b-8ed2-d135a262acbf"
      },
      "source": [
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(tweets[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.7579, 'neg': 0.0, 'neu': 0.615, 'pos': 0.385}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Tools:\n",
        "\n",
        "* [Preprocessing library for Twitter](https://github.com/s/preprocessor)\n",
        "* [Emoji library](https://github.com/carpedm20/emoji)\n",
        "* [Demoji library](https://github.com/bsolomon1124/demoji)\n",
        "\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [Lexical Normalization](https://arxiv.org/pdf/1710.03476.pdf)\n",
        "* [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://aclanthology.org/2021.naacl-main.50.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}