{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Lab overview:\n",
        "\n",
        "\n",
        "* Normalization\n",
        "* Tokenization\n",
        "* Lematization\n",
        "* Stemming\n",
        "* Stopwords removal\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nK8jcjSNkv3"
      },
      "source": [
        "##Text normalization (cleaning)\n",
        "\n",
        "Depending on the task you are cleaning the text for, you may perform one or more of: \n",
        "\n",
        "* Transform text to lowercase\n",
        "* Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "* Remove punctuation\n",
        "* Remove digits or transform them to words\n",
        "* Correct spelling errors\n",
        "\n",
        "\n",
        "Python Regular Expressions \n",
        "*   [re Python documentation](https://docs.python.org/3/library/re.html)\n",
        "*   [Quick reference](https://www.computerhope.com/unix/regex-quickref.htm)\n",
        "*   [Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)\n",
        "\n",
        "![regular_expressions](https://res.cloudinary.com/practicaldev/image/fetch/s--_iE0KvdT--/c_imagga_scale,f_auto,fl_progressive,h_900,q_auto,w_1600/https://dev-to-uploads.s3.amazonaws.com/i/zpek00ubevoxvn458b01.png)\n",
        "\n",
        "[Photo source](https://dev.to/mconner89/regular-expressions-grouping-and-string-methods-3ijn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzKaNrWZ16E"
      },
      "source": [
        "Here is our text sample, a short review of the movie [Jaws](https://en.wikipedia.org/wiki/Jaws_(film))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwbmESJGWAre"
      },
      "source": [
        "text = '\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-zwIaLJ_gI"
      },
      "source": [
        "Transform text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEhAwxLyaX7L"
      },
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htuAG8zKarGo"
      },
      "source": [
        "importing [re](https://docs.python.org/3/library/re.html) library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOeMPOhga09l"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ioMoqMP4sW"
      },
      "source": [
        "Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PM0guG_P60M"
      },
      "source": [
        "re.sub(' \\d+', '', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQEFmcfRPYc"
      },
      "source": [
        "Converting numbers to words using [num2words](https://github.com/savoirfairelinux/num2words) (it works on multiple languages)\n",
        "\n",
        "We need to install the num2words library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx3KhgnRPfv"
      },
      "source": [
        "!pip install num2words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1K45N29RlOX"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Egr4m_9Rl9b"
      },
      "source": [
        "from num2words import num2words\n",
        "\n",
        "text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
        "text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEeAr6pKFj0"
      },
      "source": [
        "Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "\n",
        "Using [emoji](https://github.com/carpedm20/emoji) library or the corresponding unicode characters.\n",
        "\n",
        "We need to install the emoji library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq72wdvEKI7z"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVu11QZEKoSW"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HYmbrutKnEo"
      },
      "source": [
        "import emoji\n",
        "\n",
        "emoji.get_emoji_regexp().sub(u'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69dNffKLtqu"
      },
      "source": [
        "The *get_emoji_regexp()* function returns a regex to match any emoji.\n",
        "\n",
        "Another way of removing emojis with regex:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSh8NUIM_bY"
      },
      "source": [
        "emoj = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2640-\\u2642\" \n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"\n",
        "    u\"\\u3030\"\n",
        "    \"]+\", re.UNICODE)\n",
        "\n",
        "text = re.sub(emoj, '', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_xacsPCOPjL"
      },
      "source": [
        "Removing emoticons (regex from [nltk Twitter Tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYkCRPNOS3n"
      },
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      </?3                       # heart\n",
        "    )\"\"\"\n",
        "    \n",
        "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "text = re.sub(emoticon_re, '', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VkbsLqXMyYo"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "*   Word level: Split by whitespace, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "*   Sentence level: Split by punctuation, [nltk.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrN047ULHZQ_"
      },
      "source": [
        "print(text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyTVP55VRwT"
      },
      "source": [
        "We need to download first the Punkt Tokenizer Models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHvjfBzxVdAv"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nZasrqFVE-h"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "tokenized_text_nltk = word_tokenize(text)\n",
        "print(tokenized_text_nltk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yypjBlMVpnG"
      },
      "source": [
        "Sentence tokenization using regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqqArIwwVp1y"
      },
      "source": [
        " re.split('(?<=[.!?]) +', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7KKoyA1WNVo"
      },
      "source": [
        "Sentence tokenization using nltk.sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3RfpgBzWSaL"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9idFgSWnJp"
      },
      "source": [
        "text_example = 'I was good.Thanks.'\n",
        "re.split('(?<=[.!?]) +', text_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0DKB0VoWr8M"
      },
      "source": [
        "nltk.sent_tokenize(text_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s4GXfHXCOl"
      },
      "source": [
        "Removing punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCxHclC0XF8e"
      },
      "source": [
        "re.sub(r'[^\\w\\s]','', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYxNlaSgi6gH"
      },
      "source": [
        "Using [string](https://docs.python.org/3/library/string.html) library. \n",
        "\n",
        "The string.punctuation method returns a list of punctuation marks. \n",
        "\n",
        "We use the translate() method which replaces every instance of a punctuation mark with the value '' in our strings. We use the str.maketrans() method to support the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jid14LagYAPQ"
      },
      "source": [
        "import string\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkRYWmakLla"
      },
      "source": [
        "Removing multiple spaces between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAFnlSzYu7_"
      },
      "source": [
        "text = re.sub(' +', ' ', text)\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F0pEMhIHbd6"
      },
      "source": [
        "## Removing stopwords\n",
        "\n",
        "![stopwords.jpg](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png) \n",
        "\n",
        "[Photo source](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980726-remove-stop-words-from-a-block-of-text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53_oLK7NP8y"
      },
      "source": [
        "###Why do we Need to Remove Stopwords?\n",
        "\n",
        "For tasks such as text classification, we may want to remove any unnecessary words and keep only words with meaning. \n",
        "\n",
        "Stopwords removal is not used in tasks such as machine translation or text summarization.\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGkYhh5oMcx1"
      },
      "source": [
        "Stopwords removal using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idPo-mzrC2HW"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY7q0410L7t0"
      },
      "source": [
        "tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3FIzVTMYc7"
      },
      "source": [
        "Stopwords removal using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys1CTPNoMgNv"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE9rct__Oi7a"
      },
      "source": [
        "tokenized_text_spacy = nlp(text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH15qHZ3HYga"
      },
      "source": [
        "## Lematization/Stemming\n",
        "\n",
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)\n",
        "\n",
        "[Photo source](https://tr.pinterest.com/pin/706854104005417976/)\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOVeBOOYZo7F"
      },
      "source": [
        "Lematization\n",
        "\n",
        "Using the WordNetLemmatizer from nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89_jM6NY_nh"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxu3jUM1UviR"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = word_tokenize(text)\n",
        "for word in words:\n",
        "    print(word, lemmatizer.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWPz08R0qD79"
      },
      "source": [
        "Using the [lemmatizer](https://spacy.io/api/lemmatizer) from spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5oA4G1muu4y"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YnUMJpqEO3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMUayDqZweHR"
      },
      "source": [
        "Stemming in using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN7dDNx1weU3"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word, ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmgsTVhIZuS9"
      },
      "source": [
        "[Other stemmers in nltk](https://www.nltk.org/api/nltk.stem.html)\n",
        "\n",
        "The spacy library does not perform stemming, only lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/ygCNwFM4i5RMPtsC6\n",
        "\n",
        "Preprocess texts from Twitter\n",
        "\n",
        "## Data\n",
        "\n",
        "We will use the twitter corpus from nltk, usually used in sentiment analysis.\n",
        "\n",
        "The fist step is downloading the dataset using the *download* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fLAUv1MRdHq"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_VdfEmjKXh"
      },
      "source": [
        "In order to inspect our data, we look at the first 25 tweets from the dataset. The text contains a lot of mentions, hashtags and emoticons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIODYYSvSuWj"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "tweets = tweets[:25]\n",
        "tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V_Y5pHtc07i"
      },
      "source": [
        "**Given a list of tweets, preprocess each tweet from the list.**\n",
        "\n",
        "**Instructions**: Implement the *preprocess* function. You can do the text cleaning in any order you prefer.\n",
        "\n",
        "**Hint**: You may need to use regex expressions (use the resources provided above).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAmrMgS7QA"
      },
      "source": [
        "def preprocess(tweets):\n",
        "\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        tweets: a list of tweets\n",
        "    Output: \n",
        "        prepocessed_tweets: a list of preprocessed tweets\n",
        "    \"\"\"\n",
        "\n",
        "    ###you may need to create an additional list in which to store the processed tweets\n",
        "    ###pay attention that some of the cleaning steps can be done at the document level, while others may be computed at word level\n",
        "\n",
        "\n",
        "    for tweet in tweets:\n",
        "\n",
        "        ###remove new line characters '\\n'\n",
        "        ###remove links http://t.co/of3DyOzML0\n",
        "        ###remove mentions '@'\n",
        "        ###remove hashtags '#'\n",
        "        ###lowercase text\n",
        "        ###remove emojis and emoticons 'üëå üç≠ :) :D'\n",
        "        ###remove digits\n",
        "        ###remove punctuation\n",
        "        ###tokenize tweet into separate words\n",
        "        ###remove stopwords\n",
        "        ###lematization or stemming\n",
        "    \n",
        "    return prepocessed_tweets\n",
        "\n",
        "preprocess(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Tools:\n",
        "\n",
        "* [Preprocessing library for Twitter](https://github.com/s/preprocessor)\n",
        "* [Emoji library](https://github.com/carpedm20/emoji)\n",
        "* [Demoji library](https://github.com/bsolomon1124/demoji)\n",
        "* [Gensim](https://radimrehurek.com/gensim/)\n",
        "\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [Lexical Normalization](https://arxiv.org/pdf/1710.03476.pdf)\n",
        "* [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://aclanthology.org/2021.naacl-main.50.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}