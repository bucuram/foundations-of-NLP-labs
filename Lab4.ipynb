{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfSyq15S2ax2fMLtJfwFl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcYhGNPflMSx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXkuh5_mj3Wx"
      },
      "source": [
        "##Word2vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJqjzaGlQnT"
      },
      "source": [
        "Word representation methods from the last lab\n",
        "\n",
        "- Bag of Words\n",
        "- TF-IDF\n",
        "\n",
        "Limitations of these representations\n",
        "\n",
        "- High-dimensional\n",
        "- Sparse\n",
        "- No info about words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GPodGvnlyZX"
      },
      "source": [
        "Word2vec Paper [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkQ87w_smCcg"
      },
      "source": [
        "Word2Vec is a shallow, two-layer neural network which is trained to reconstruct linguistic contexts of words.\n",
        "\n",
        "It takes as its input a large corpus of words and produces a vector space, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
        "\n",
        "\n",
        "Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
        "\n",
        "Example:    \n",
        "The **kid** studies mathematics.\n",
        "\n",
        "The **child** studies mathematics.\n",
        "\n",
        "![embedding](https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7oue8cDnY1_"
      },
      "source": [
        "###Methods for building the Word2vec model\n",
        "\n",
        "![cbow-skip-gram](https://miro.medium.com/max/1400/1*cuOmGT7NevP9oJFJfVpRKA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KUil7cBnY4u"
      },
      "source": [
        "###Continuous Bag-of-Words (CBOW)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb12NPvv3Zww"
      },
      "source": [
        "CBOW predicts target words from the surrounding context words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfz-eLcNnY7f"
      },
      "source": [
        "![cbow](https://1.bp.blogspot.com/-nZFc7P6o3Yc/XQo2cYPM_ZI/AAAAAAAABxM/XBqYSa06oyQ_sxQzPcgnUxb5msRwDrJrQCLcBGAs/s1600/image001.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZpZNbVynY91"
      },
      "source": [
        "###Skip-gram\n",
        "\n",
        "Skip-gram predicts surrounding context words from the target words.\n",
        "\n",
        "![skip-gram](https://i.stack.imgur.com/fYhXF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GGcsuG3s9_"
      },
      "source": [
        "##Architecture\n",
        "\n",
        "The words are feeded as one-hot vectors ( vector of the same length as the vocabulary, filled with zeros except at the index that represents the word we want to represent, which is assigned “1”.)\n",
        "\n",
        "The hidden layer is a standard fully-connected (Dense) layer whose weights are the word embeddings.\n",
        "\n",
        "The output layer outputs probabilities for the target words from the vocabulary.\n",
        "\n",
        "The goal of this neural network is to learn the weights for the hidden layer matrix.\n",
        "\n",
        "![model](https://miro.medium.com/max/1400/1*tmyks7pjdwxODh5-gL3FHQ.png)\n",
        "\n",
        "High-level illustration of the architecture\n",
        "\n",
        "![model2](https://i.imgur.com/CBuZay5.png)\n",
        "\n",
        "The rows of the hidden layer weight matrix, are actually the word vectors (word embeddings).\n",
        "\n",
        "\n",
        "![hidden-layer](https://i.imgur.com/v6VqHad.png)\n",
        "\n",
        "The hidden layer operates as a lookup table. The output of the hidden layer is just the “word vector” for the input word.\n",
        "\n",
        "More concretely, if you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the ‘1’.\n",
        "\n",
        "![vector](https://i.imgur.com/EYhcA5S.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCMFRaI6S2a"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZgTsN3Pmye0"
      },
      "source": [
        "###Semantic and syntactic relationships\n",
        "\n",
        "If different words are similar in context, then Word2Vec should have similar outputs when these words are passed as inputs, and in-order to have a similar outputs, the computed word vectors (in the hidden layer) for these words have to be similar, thus Word2Vec is motivated to learn similar word vectors for words in similar context.\n",
        "\n",
        "Word2Vec is able to capture multiple different degrees of similarity between words, such that semantic and syntactic patterns can be reproduced using vector arithmetic.\n",
        "\n",
        "![w2vec](https://i.imgur.com/I66L7No.png)\n",
        "\n",
        "![w2vec2](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/linear-relationships.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Now1mZyS_dYe"
      },
      "source": [
        "**Skip-gram** - works well with a small amount of the training data, represents well even rare words or phrases\n",
        "\n",
        "**CBOW** - several times faster to train than the skip-gram, slightly better accuracy for the frequent words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSzy26Qw_dc5"
      },
      "source": [
        "###Word2vec embeddings in Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuFIQwiLQkB5"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BUnYjhKAWoI"
      },
      "source": [
        "Gensim has multiple vector representations for words: word2vec, fasttext, glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7rV7zCyAHdJ",
        "outputId": "64457c32-9df1-46fe-a156-45c0771c48f4"
      },
      "source": [
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo0dnaGaAesc"
      },
      "source": [
        "Downloading the word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XBIE2c_Aifx"
      },
      "source": [
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acjBn5fQRoxb",
        "outputId": "4e4769a3-d17f-4df5-938b-a3b5e2be96df"
      },
      "source": [
        "word2vec['cat'][:20]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
              "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
              "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
              "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6-ySA4OBK3P",
        "outputId": "8cd82873-fda3-4b0e-e9d8-e324948865af"
      },
      "source": [
        "word2vec.similarity('dog', 'house')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25689757"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMjYXayBNB1",
        "outputId": "1dae50cc-14a2-46b5-d1b8-c67ce18c0548"
      },
      "source": [
        "word2vec.similarity('dog', 'puppy')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81064284"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYtBf4mfBPsj",
        "outputId": "1155061f-d662-462b-b03d-96358f36a4a9"
      },
      "source": [
        "word2vec.most_similar('cat')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cats', 0.8099379539489746),\n",
              " ('dog', 0.7609456777572632),\n",
              " ('kitten', 0.7464985251426697),\n",
              " ('feline', 0.7326233983039856),\n",
              " ('beagle', 0.7150583267211914),\n",
              " ('puppy', 0.7075453996658325),\n",
              " ('pup', 0.6934291124343872),\n",
              " ('pet', 0.6891531348228455),\n",
              " ('felines', 0.6755931377410889),\n",
              " ('chihuahua', 0.6709762215614319)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xixp7uGyEMTH"
      },
      "source": [
        "\n",
        "(king - man) + woman = queen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sms3zhUKBRVH",
        "outputId": "04ef1bdd-c07b-4470-94a6-62a4a056210b"
      },
      "source": [
        "word2vec.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321243286133),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.518113374710083),\n",
              " ('sultan', 0.5098593235015869),\n",
              " ('monarchy', 0.5087411999702454)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIs9pHp3BZTT"
      },
      "source": [
        "##Assignemnet\n",
        "\n",
        "To be uploaded here: https://forms.gle/KuR71xiA2rR6tukz8 until December 1st.\n",
        "\n",
        "1. Play around with the word2vec model and see if there are any interesting or counterintuitive similarity results using  ```word2vec.similarity``` and ```word2vec.most_similar```.\n",
        "\n",
        "2. Use word2vec embeddings to encode the data from the sentiment analysis task from Lab 3 and train a classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftFqAxWFlT_J"
      },
      "source": [
        "Adapted from https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n",
        "\n",
        "Further Reading\n",
        "\n",
        "- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)"
      ]
    }
  ]
}