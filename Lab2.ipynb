{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing for non-English languages\n",
        "\n",
        "##Challenges\n",
        "* Diacritics restoration \n",
        "* Text segmentation for Chinese, Japanese, Arabic\n",
        "* You may need to have some knowledge about the language\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eee1lFEWQgq9"
      },
      "source": [
        "##Case studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu6CzKPSQof-"
      },
      "source": [
        "###Romanian\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nWQUPlZaKR7"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "romanian_text = '''Examenul Chunin începe 25. A zecea întrebare: Totul sau nimic!\n",
        "    Excepţie a făcut-o Hotinul, unde s-a aşezat o garnizoană turcească, care a început să jefuiască cu cruzime ţara.\n",
        "    Excitarea catatonică este o stare de agitaţie constantă şi de excitare motrică şi nervoasă.\n",
        "    Excreţiile parazitului irită pielea prpducând mâncărime şi răni produse de scărpinat, iau naştere papule, vezicule, pustule, infiltraţii, acestea prin infecţii secundare, se pot transforma în furunculi.\n",
        "    Executarea tabloului este precedată de o întreagă serie de schiţe.\n",
        "    Execuţia acestei lucrări a început în anul 1957 şi s-a finalizat în anul urmator.\n",
        "    Exemple de mesaje au fost eliminarea unui al doilea membru al tribului după ce a fost eliminat unul sau evitarea participării la Consiliul Tribal, dar cu preţul mutării intr-o locaţie mai puţin confortabilă.\n",
        "    Exemple de specii din grupul calmarilor pot servi Loligo vulgaris, Ommastrephes sagittatus, Ommastrephes slosnei pacificus, Chiroteuthis veranyi etc. Unele specii sunt exploatate pentru carnea lor comestibilă\n",
        "    '''\n",
        "pprint(romanian_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SwK7ZIa3Gt"
      },
      "source": [
        "Lowercasing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIrLujmWa5Mb"
      },
      "source": [
        "romanian_text = romanian_text.lower()\n",
        "pprint(romanian_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBJNRbnzbAIe"
      },
      "source": [
        "Removing digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmFfE5VSbCHo"
      },
      "source": [
        "import re\n",
        "\n",
        "romanian_text = re.sub(' \\d+', '', romanian_text)\n",
        "pprint(romanian_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3vte5EFb7x0"
      },
      "source": [
        "Removing diacritics using [Unidecode](https://pypi.org/project/Unidecode/). Transforming Unicode text in ASCII."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxU-1bXcGBv"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2LOnLH5b9nZ"
      },
      "source": [
        "import unidecode\n",
        "romanian_text  = unidecode.unidecode(romanian_text)\n",
        "pprint(romanian_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu_mud4-bMoA"
      },
      "source": [
        "Sentence tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9KEQgmbRey"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sent_romanian = sent_tokenize(romanian_text)\n",
        "sent_romanian "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAhei-_DbuMB"
      },
      "source": [
        "Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZSvPmlVdYZH"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import string\n",
        "\n",
        "\n",
        "sent_romanian = [sent.translate(str.maketrans('', '', string.punctuation)) for sent in sent_romanian]\n",
        "\n",
        "words_romanian = [[word_tokenize(sent) for sent in sent_romanian]]\n",
        "pprint(words_romanian) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14iGgZq-7cW"
      },
      "source": [
        "spaCy on Romanian text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjn9U8a5Qqxm"
      },
      "source": [
        "###Japanese \n",
        "\n",
        "[Japanese punctuation](https://en.wikipedia.org/wiki/Japanese_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajsK1qxrGu4S"
      },
      "source": [
        "japanese_text = '''$100ベットしていた場合には$100の配当を得られる。\n",
        "    なお、100形のうち101 - 110の10両は当時丸ノ内線方南町支線用であり 、これの代替には1500N形投入により2000形を10両 (2031 - 2040) 捻出して対応している。\n",
        "    コンセプトは「100時間遊べるおまけ」。\n",
        "    なお100系1･2次車の床面高さは従前通りの1150mmである。\n",
        "    ゴチ10・12・15ではMCを担当。\n",
        "    ! 101番目の魔物 （ 大海恵 ） 2005年 * 劇場版 金色のガッシュベル!\n",
        "    1088年に誕生した人物及び著名な動物 。'''"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMzZlhIcHaDM"
      },
      "source": [
        "Removing digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At5P_Fd-Hb_y"
      },
      "source": [
        "import re\n",
        "\n",
        "japanese_text = ''.join([c for c in japanese_text if c.isdigit() == False ])\n",
        "japanese_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zid78ObFMwS_"
      },
      "source": [
        "Installing some dependencies for japanese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBN4bFN0gB48"
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXhJJWhNgS-b"
      },
      "source": [
        "!pip install fugashi[unidic-lite]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua13p15FEPIR"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ja_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0mRJHgKGefO"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apvAwCqTEh94"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwrehuGrGbqx"
      },
      "source": [
        "tokenized_text_spacy = nlp(japanese_text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywO8_YjJXXr"
      },
      "source": [
        "Removing punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g4dBNDwgMHy"
      },
      "source": [
        "import spacy\n",
        "for word in tokenized_text_without_stopwords:\n",
        "    if word.is_punct:\n",
        "        print(word, word.lemma_, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO2krQx8JZZe"
      },
      "source": [
        "for word in tokenized_text_without_stopwords:\n",
        "    if word.is_punct == False:\n",
        "        print(word, word.lemma_, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/wLsjCWnxK7w8GPvt9\n",
        "\n",
        "Choose samples from 2 languages and preprocess the texts (normalization, tokenization, lematization, etc.).\n",
        "\n",
        "Try to use spacy or find other resources and tools for the chosen languages.\n",
        "\n",
        "Also mention if the language you have choosen needs specific preprocessing.\n",
        "\n",
        "**Please add the resources you used in the doc**: https://docs.google.com/document/d/1c5sqPfgSioGzLZkRv4yWw7DWiiQC96QHw34ZRqcDiSY/edit?usp=sharing for further reference.\n",
        "\n",
        "\n",
        "Questions: If you have chosen a language you are fluent in, how well did the tools work on this language? What problems did you observed (e.g. problems with tokenization, etc.)\n",
        "\n",
        "## Data\n",
        "\n",
        "Data can be downloaded from the resources below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNZ0qRZ1JlIJ"
      },
      "source": [
        "###write the code for your assignment here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Resources:\n",
        "\n",
        "* [An Crúbadán - Corpus Building for Minority Languages - 18721 languages](http://crubadan.org/)\n",
        "* [Leipzig Corpora Collection / Deutscher Wortschatz 291 languages](https://wortschatz.uni-leipzig.de/en/download)\n",
        "* [Europarl](https://www.statmt.org/europarl/)\n",
        "\n",
        "Tools:\n",
        "* [Chinese text segmentation](https://github.com/fxsjy/jieba)\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [A Survey of Approaches to Diacritic Restoration](https://www.researchgate.net/profile/Franklin-Asahiah/publication/328419851_A_Survey_of_Approaches_to_Diacritic_Restoration/links/5bcd8b67458515f7d9d02f3d/A-Survey-of-Approaches-to-Diacritic-Restoration.pdf)\n",
        "* [Preprocessing Arabic text on social media](https://www.sciencedirect.com/science/article/pii/S2405844021002966)\n",
        "* [Semantic-Based Segmentation of Arabic Texts](https://scialert.net/fulltext/?doi=itj.2008.1009.1015)\n",
        "* [The case of Croatian](https://medium.com/krakensystems-blog/text-processing-problems-with-non-english-languages-82822d0945dd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}