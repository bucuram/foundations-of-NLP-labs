{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbAAQm+Qk5kjOunQGvvk6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ0vy1SifWLS"
      },
      "source": [
        "#Word Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd2CqfL-f1nU"
      },
      "source": [
        "###Bag of Words\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. \n",
        "\n",
        "It is called a ***bag*** of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
        "\n",
        "If your dataset is small and context is domain specific, BoW may work better than Word Embedding because you may not find the corresponding vector from pre-trained word embedding models for some of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OkGW5V20vpo"
      },
      "source": [
        "![bow](https://miro.medium.com/max/554/0*B9GC_f3BMtjGMdQ-.png)\n",
        "\n",
        "[Photo source](https://medium.com/analytics-vidhya/does-tf-idf-work-differently-in-textbooks-and-sklearn-routine-cc7a7d1b580d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixAb90cV2smS",
        "outputId": "b1f18d90-7e4e-4628-97fb-6de2e8285a35"
      },
      "source": [
        "corpus = [\"Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.\",\n",
        "    \"The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.\",\n",
        "    \"Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.\"]\n",
        "\n",
        "corpus"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.',\n",
              " 'The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.',\n",
              " 'Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMH00eL4WC1"
      },
      "source": [
        "We will create the BoW vectors using `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnfTYi2U3qi6",
        "outputId": "4749edc1-06d2-4a41-b636-4eac77ca5c66"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase = False)\n",
        "bow_representation = vectorizer.fit_transform(corpus)\n",
        "vocabulary = vectorizer.get_feature_names()\n",
        "\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))\n",
        "print(bow_representation.toarray())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flora', 'Sometimes', 'The', 'all', 'also', 'and', 'animal', 'are', 'as', 'bacteria', 'biota', 'collectively', 'corresponding', 'fauna', 'flora', 'for', 'forms', 'fungi', 'generally', 'gut', 'in', 'indigenous', 'is', 'life', 'native', 'naturally', 'occurring', 'of', 'or', 'other', 'particular', 'plant', 'plants', 'present', 'referred', 'region', 'skin', 'such', 'term', 'terms', 'the', 'time', 'to']\n",
            "43\n",
            "[[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            "  0 0 0 0 2 1 0]\n",
            " [1 0 1 0 0 1 1 1 2 0 1 1 1 2 0 1 1 1 0 0 0 0 1 2 0 0 0 1 0 1 0 0 0 0 1 0\n",
            "  0 1 1 0 0 0 1]\n",
            " [0 1 0 0 1 1 0 1 2 1 0 0 0 0 3 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            "  1 0 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF5RZAfAJoRl"
      },
      "source": [
        "####N-grams encoding\n",
        "\n",
        "Extracts features from text while capturing local word order by defining\n",
        "counts over sliding windows.\n",
        "\n",
        "![ngrams](https://i.stack.imgur.com/8ARA1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IhluFw_KceT",
        "outputId": "8d931344-ecd2-405d-d323-a5c62baccac8"
      },
      "source": [
        "bigram = CountVectorizer(lowercase = False, ngram_range=(2, 2))\n",
        "bigram_representation = bigram.fit_transform(corpus)\n",
        "\n",
        "bigram_vocabulary = bigram.get_feature_names()\n",
        "\n",
        "print(bigram_vocabulary)\n",
        "print(len(bigram_vocabulary))\n",
        "print(bigram_representation.toarray())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flora fauna', 'Flora is', 'Sometimes bacteria', 'The corresponding', 'all the', 'also referred', 'and fungi', 'and other', 'animal life', 'are also', 'are collectively', 'as biota', 'as flora', 'as fungi', 'as in', 'bacteria and', 'collectively referred', 'corresponding term', 'fauna Flora', 'fauna and', 'flora as', 'flora or', 'for animal', 'forms of', 'fungi are', 'generally the', 'gut flora', 'in particular', 'in the', 'indigenous native', 'is all', 'is fauna', 'life is', 'life present', 'life such', 'native plants', 'naturally occurring', 'occurring indigenous', 'of life', 'or skin', 'or time', 'other forms', 'particular region', 'plant life', 'present in', 'referred to', 'region or', 'skin flora', 'such as', 'term for', 'terms gut', 'the naturally', 'the plant', 'the terms', 'time generally', 'to as']\n",
            "56\n",
            "[[0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            "  1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0]\n",
            " [1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
            "  0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vLypvTLff-H"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOMJQfEQTE74"
      },
      "source": [
        "TF-IDF represents\n",
        "text data by indicating the importance of the word relative to the other words in\n",
        "the text.\n",
        "\n",
        "A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much ‚Äúinformational content‚Äù to the model as rarer but perhaps domain specific words.\n",
        "\n",
        "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like ‚Äúthe‚Äù that are also frequent across all documents are penalized.\n",
        "\n",
        "This approach to scoring is called Term Frequency ‚Äì Inverse Document Frequency, or TF-IDF for short, where:\n",
        "\n",
        "- **Term Frequency**: the frequency of a given term in a document.\n",
        "\n",
        "\n",
        "\n",
        "- **Inverse Document Frequency**: the ratio of documents that contain a given term.\n",
        "\n",
        "![tf](https://www.affde.com/uploads/article/5516/PVpklt43xBCKRFBa.png)\n",
        "\n",
        "TF-IDF penalizes stopwords, they will not have a high score, but stopwords removal may stil be used to reduce the dimensionality of the input space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2lVOt6uSxno"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uALv5QN0Swxc",
        "outputId": "c86af754-db12-485a-823f-2628f383f657"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
        "tfidf_representation = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(tfidf_representation.toarray())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.18334923 0.         0.         0.2410822  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.2410822  0.         0.18334923 0.2410822  0.18334923 0.18334923\n",
            "  0.2410822  0.2410822  0.2410822  0.         0.18334923 0.\n",
            "  0.2410822  0.2410822  0.2410822  0.2410822  0.         0.2410822\n",
            "  0.         0.         0.         0.         0.36669846 0.2410822\n",
            "  0.        ]\n",
            " [0.15630031 0.         0.20551613 0.         0.         0.15630031\n",
            "  0.20551613 0.15630031 0.31260063 0.         0.20551613 0.20551613\n",
            "  0.20551613 0.41103226 0.         0.20551613 0.20551613 0.15630031\n",
            "  0.         0.         0.         0.         0.15630031 0.31260063\n",
            "  0.         0.         0.         0.20551613 0.         0.20551613\n",
            "  0.         0.         0.         0.         0.15630031 0.\n",
            "  0.         0.20551613 0.20551613 0.         0.         0.\n",
            "  0.15630031]\n",
            " [0.         0.21348818 0.         0.         0.21348818 0.16236326\n",
            "  0.         0.16236326 0.32472653 0.21348818 0.         0.\n",
            "  0.         0.         0.64046454 0.         0.         0.16236326\n",
            "  0.         0.21348818 0.16236326 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16236326 0.\n",
            "  0.         0.         0.         0.         0.16236326 0.\n",
            "  0.21348818 0.         0.         0.21348818 0.16236326 0.\n",
            "  0.16236326]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBRqxWPySXvI"
      },
      "source": [
        "####Limitations\n",
        "\n",
        "- **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
        "\n",
        "- **Sparsity:** Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (‚Äúthis is interesting‚Äù vs ‚Äúis this interesting‚Äù), synonyms (‚Äúold bike‚Äù vs ‚Äúused bike‚Äù), and much more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8pLENc-usC-"
      },
      "source": [
        "##Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ21VYBRJhz2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzQ-xXrwQ4Z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJnbhFjtzEtu",
        "outputId": "f57624eb-0a7b-4f13-9a08-40ad3f08d387"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ltO6u9bbWk",
        "outputId": "35c2fa39-f03e-4873-d05a-e76f486ff5bc"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "print(len(pos_tweets))\n",
        "\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "print(len(neg_tweets))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPa_1MLnccON"
      },
      "source": [
        "import pandas as pd\n",
        "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
        "pos_df['label'] = 1"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0LpGyddQ5H"
      },
      "source": [
        "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
        "neg_df['label'] = 0"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-dAAOa9WdalL",
        "outputId": "a62fe4e2-ac91-4b69-89b5-92fca19f2418"
      },
      "source": [
        "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "data_df"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@97sides CONGRATS :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>I wanna change my avi but uSanele :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>where's all the jaebum baby pictures :((</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  label\n",
              "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...      1\n",
              "1     @Lamb2ja Hey James! How odd :/ Please call our...      1\n",
              "2     @DespiteOfficial we had a listen last night :)...      1\n",
              "3                                  @97sides CONGRATS :)      1\n",
              "4     yeaaaah yippppy!!!  my accnt verified rqst has...      1\n",
              "...                                                 ...    ...\n",
              "9995               I wanna change my avi but uSanele :(      0\n",
              "9996                         MY PUPPY BROKE HER FOOT :(      0\n",
              "9997           where's all the jaebum baby pictures :((      0\n",
              "9998  But but Mr Ahmad Maslan cooks too :( https://t...      0\n",
              "9999  @eawoman As a Hull supporter I am expecting a ...      0\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiLCqzVtvC18"
      },
      "source": [
        "####Split data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhrzAx8ZduUU",
        "outputId": "5b9889c3-9942-4ad6-bb4c-20f9ad1891c6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
        "print(train_df)\n",
        "print(test_df)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  tweet  label\n",
            "3520  @MusicMetrop @LostInMuzic @karenak @MosesMo @c...      1\n",
            "9447                           @sugarymgc i miss you :(      0\n",
            "8347                        Miss u :-( @deepikapadukone      0\n",
            "2274  @sculptorfred You're doing well for a beginner...      1\n",
            "9425                            I miss my boyfriend :-(      0\n",
            "...                                                 ...    ...\n",
            "5669  So Much New Music that I cant record yet coz m...      0\n",
            "3601  üíÉüíÉüíÉ\"@Boitumelo_SA: @Roooosta happy birthday to...      1\n",
            "7579  @itsNotMirna so true, I voted for them so many...      0\n",
            "2530  @savkra wow. So much hate came your way for sp...      1\n",
            "9603                                      I need hug :(      0\n",
            "\n",
            "[8000 rows x 2 columns]\n",
            "                                                  tweet  label\n",
            "6186  @RichelleMead it's being more than enough sinc...      0\n",
            "6326  @VMilas dont  really know sorry.:( try opening...      0\n",
            "4241  @katie_taylorkay yes omg. I lile tid more than...      1\n",
            "4932        yah know I'm good at faking mah emotions :)      1\n",
            "1539  Hi @StefanieScott i hope you're having a great...      1\n",
            "...                                                 ...    ...\n",
            "5543  @enikotsz @pooj_ @UberUK @walls With this rain...      0\n",
            "9763  Char im really sick :(( one *font size 8* mini...      0\n",
            "851   @aubreysablan I come back next Thursday!! We c...      1\n",
            "8496                     @beelovley19 wow hate u :( imy      0\n",
            "7137  I hate being up late bc then I won't wake up e...      0\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hENqcGGeBjx"
      },
      "source": [
        "####TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXgh5E4PeEdx"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
        "tfidf_representation = tfidf_vectorizer.fit(train_df['tweet'])\n",
        "\n",
        "X_train = tfidf_vectorizer.transform(train_df['tweet'])\n",
        "X_test = tfidf_vectorizer.transform(test_df['tweet'])\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMMWmLnegV50"
      },
      "source": [
        "Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SleRBR3Gfcx2",
        "outputId": "d0be8098-76b0-456e-b8af-6a88d5232de5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaRmuGBPfqH8",
        "outputId": "e22f1cad-05fc-444e-f077-35e35fa3db54"
      },
      "source": [
        "print('Train Score', logreg.score(X_train, y_train))\n",
        "print('Test Score', logreg.score(X_test, y_test))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score 0.8995\n",
            "Test Score 0.7675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAsQmZ9Q5J_"
      },
      "source": [
        "##Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/qTzLy6F6jkUtQrvy7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj_ckmB_Q8JS"
      },
      "source": [
        "Investigate the effect of text normalization.\n",
        "\n",
        "- Search for a dataset for classification (or experiment with the same dataset from this lab)\n",
        "- Preprocess the text\n",
        "- Compare the vocabulary size with and without preprocessing\n",
        "- Get the numerical representation of the text\n",
        "- Train a model\n",
        "- Test your model \n",
        "- Compare the performance of your model with and without text normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Se1cpufQun"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}