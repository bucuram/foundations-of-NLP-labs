{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcqFrvCoUK5nKgWZiWnHrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/foundations-of-NLP-labs/blob/main/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ0vy1SifWLS"
      },
      "source": [
        "#Word Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd2CqfL-f1nU"
      },
      "source": [
        "###Bag of Words\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. \n",
        "\n",
        "It is called a ***bag*** of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
        "\n",
        "If your dataset is small and context is domain specific, BoW may work better than Word Embedding because you may not find the corresponding vector from pre-trained word embedding models for some of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OkGW5V20vpo"
      },
      "source": [
        "![bow](https://miro.medium.com/max/554/0*B9GC_f3BMtjGMdQ-.png)\n",
        "\n",
        "[Photo source](https://medium.com/analytics-vidhya/does-tf-idf-work-differently-in-textbooks-and-sklearn-routine-cc7a7d1b580d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixAb90cV2smS",
        "outputId": "f25d3634-fbc6-4b93-9dcb-3554ea847114"
      },
      "source": [
        "corpus = [\"Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.\",\n",
        "    \"The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.\",\n",
        "    \"Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.\"]\n",
        "\n",
        "corpus"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Flora is all the plant life present in a particular region or time, generally the naturally occurring (indigenous) native plants.',\n",
              " 'The corresponding term for animal life is fauna. Flora, fauna, and other forms of life, such as fungi, are collectively referred to as biota.',\n",
              " 'Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMH00eL4WC1"
      },
      "source": [
        "We will create the BoW vectors using `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnfTYi2U3qi6",
        "outputId": "e3334d72-ba7a-49d4-b7aa-85825bc75c9a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase = False)\n",
        "bow_representation = vectorizer.fit_transform(corpus)\n",
        "vocabulary = vectorizer.get_feature_names()\n",
        "\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))\n",
        "print(bow_representation.toarray())"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flora', 'Sometimes', 'The', 'all', 'also', 'and', 'animal', 'are', 'as', 'bacteria', 'biota', 'collectively', 'corresponding', 'fauna', 'flora', 'for', 'forms', 'fungi', 'generally', 'gut', 'in', 'indigenous', 'is', 'life', 'native', 'naturally', 'occurring', 'of', 'or', 'other', 'particular', 'plant', 'plants', 'present', 'referred', 'region', 'skin', 'such', 'term', 'terms', 'the', 'time', 'to']\n",
            "43\n",
            "[[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            "  0 0 0 0 2 1 0]\n",
            " [1 0 1 0 0 1 1 1 2 0 1 1 1 2 0 1 1 1 0 0 0 0 1 2 0 0 0 1 0 1 0 0 0 0 1 0\n",
            "  0 1 1 0 0 0 1]\n",
            " [0 1 0 0 1 1 0 1 2 1 0 0 0 0 3 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            "  1 0 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF5RZAfAJoRl"
      },
      "source": [
        "####N-grams encoding\n",
        "\n",
        "Extracts features from text while capturing local word order by defining\n",
        "counts over sliding windows.\n",
        "\n",
        "![ngrams](https://i.stack.imgur.com/8ARA1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IhluFw_KceT",
        "outputId": "a9419f0d-1555-43ff-fc8a-1beee53e1ec5"
      },
      "source": [
        "bigram = CountVectorizer(lowercase = False, ngram_range=(2, 2))\n",
        "bigram_representation = bigram.fit_transform(corpus)\n",
        "\n",
        "bigram_vocabulary = bigram.get_feature_names()\n",
        "\n",
        "print(bigram_vocabulary)\n",
        "print(len(bigram_vocabulary))\n",
        "print(bigram_representation.toarray())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flora fauna', 'Flora is', 'Sometimes bacteria', 'The corresponding', 'all the', 'also referred', 'and fungi', 'and other', 'animal life', 'are also', 'are collectively', 'as biota', 'as flora', 'as fungi', 'as in', 'bacteria and', 'collectively referred', 'corresponding term', 'fauna Flora', 'fauna and', 'flora as', 'flora or', 'for animal', 'forms of', 'fungi are', 'generally the', 'gut flora', 'in particular', 'in the', 'indigenous native', 'is all', 'is fauna', 'life is', 'life present', 'life such', 'native plants', 'naturally occurring', 'occurring indigenous', 'of life', 'or skin', 'or time', 'other forms', 'particular region', 'plant life', 'present in', 'referred to', 'region or', 'skin flora', 'such as', 'term for', 'terms gut', 'the naturally', 'the plant', 'the terms', 'time generally', 'to as']\n",
            "56\n",
            "[[0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            "  1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0]\n",
            " [1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
            "  0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vLypvTLff-H"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOMJQfEQTE74"
      },
      "source": [
        "TF-IDF represents\n",
        "text data by indicating the importance of the word relative to the other words in\n",
        "the text.\n",
        "\n",
        "A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.\n",
        "\n",
        "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n",
        "\n",
        "This approach to scoring is called Term Frequency – Inverse Document Frequency, or TF-IDF for short, where:\n",
        "\n",
        "- **Term Frequency**: the frequency of a given term in a document.\n",
        "\n",
        "\n",
        "\n",
        "- **Inverse Document Frequency**: the ratio of documents that contain a given term.\n",
        "\n",
        "![tf](https://www.affde.com/uploads/article/5516/PVpklt43xBCKRFBa.png)\n",
        "\n",
        "TF-IDF penalizes stopwords, they will not have a high score, but stopwords removal may stil be used to reduce the dimensionality of the input space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uALv5QN0Swxc",
        "outputId": "8c6b7da0-4f34-4503-e9d4-63a50a14a7c2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
        "tfidf_representation = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_representation.toarray())"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.18334923 0.         0.         0.2410822  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.2410822  0.         0.18334923 0.2410822  0.18334923 0.18334923\n",
            "  0.2410822  0.2410822  0.2410822  0.         0.18334923 0.\n",
            "  0.2410822  0.2410822  0.2410822  0.2410822  0.         0.2410822\n",
            "  0.         0.         0.         0.         0.36669846 0.2410822\n",
            "  0.        ]\n",
            " [0.15630031 0.         0.20551613 0.         0.         0.15630031\n",
            "  0.20551613 0.15630031 0.31260063 0.         0.20551613 0.20551613\n",
            "  0.20551613 0.41103226 0.         0.20551613 0.20551613 0.15630031\n",
            "  0.         0.         0.         0.         0.15630031 0.31260063\n",
            "  0.         0.         0.         0.20551613 0.         0.20551613\n",
            "  0.         0.         0.         0.         0.15630031 0.\n",
            "  0.         0.20551613 0.20551613 0.         0.         0.\n",
            "  0.15630031]\n",
            " [0.         0.21348818 0.         0.         0.21348818 0.16236326\n",
            "  0.         0.16236326 0.32472653 0.21348818 0.         0.\n",
            "  0.         0.         0.64046454 0.         0.         0.16236326\n",
            "  0.         0.21348818 0.16236326 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16236326 0.\n",
            "  0.         0.         0.         0.         0.16236326 0.\n",
            "  0.21348818 0.         0.         0.21348818 0.16236326 0.\n",
            "  0.16236326]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBRqxWPySXvI"
      },
      "source": [
        "####Limitations\n",
        "\n",
        "- **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
        "\n",
        "- **Sparsity:** Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8pLENc-usC-"
      },
      "source": [
        "##Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJnbhFjtzEtu",
        "outputId": "e8e890f1-d25d-41a1-f272-d46db5cd5685"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ltO6u9bbWk",
        "outputId": "4ff9c70d-fe21-45f7-f39b-8820a9e29cdc"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "print(len(pos_tweets))\n",
        "\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "print(len(neg_tweets))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPa_1MLnccON"
      },
      "source": [
        "import pandas as pd\n",
        "pos_df = pd.DataFrame(pos_tweets, columns = ['tweet'])\n",
        "pos_df['label'] = 1"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0LpGyddQ5H"
      },
      "source": [
        "neg_df = pd.DataFrame(neg_tweets, columns = ['tweet'])\n",
        "neg_df['label'] = 0"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-dAAOa9WdalL",
        "outputId": "a8793a2c-2116-4872-d395-e947d0d40e4e"
      },
      "source": [
        "data_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "data_df"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@97sides CONGRATS :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>I wanna change my avi but uSanele :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>where's all the jaebum baby pictures :((</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  label\n",
              "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...      1\n",
              "1     @Lamb2ja Hey James! How odd :/ Please call our...      1\n",
              "2     @DespiteOfficial we had a listen last night :)...      1\n",
              "3                                  @97sides CONGRATS :)      1\n",
              "4     yeaaaah yippppy!!!  my accnt verified rqst has...      1\n",
              "...                                                 ...    ...\n",
              "9995               I wanna change my avi but uSanele :(      0\n",
              "9996                         MY PUPPY BROKE HER FOOT :(      0\n",
              "9997           where's all the jaebum baby pictures :((      0\n",
              "9998  But but Mr Ahmad Maslan cooks too :( https://t...      0\n",
              "9999  @eawoman As a Hull supporter I am expecting a ...      0\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiLCqzVtvC18"
      },
      "source": [
        "####Split data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhrzAx8ZduUU",
        "outputId": "5fd49fa9-7dbe-4453-841c-277b0deab8f4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(data_df, test_size=0.2, shuffle = True)\n",
        "print(train_df)\n",
        "print(test_df)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  tweet  label\n",
            "8693  @cIaricestarling i know right... :( i hope it'...      0\n",
            "8098  Struggling like crazy to get into a race fan m...      0\n",
            "9132                   7pm on a Friday and I am dead :(      0\n",
            "4439  @AsianMeerkat @johncrossmirror far from it. Be...      1\n",
            "665   @FooWhiter Ugh. I've never Rt or fade any of y...      1\n",
            "...                                                 ...    ...\n",
            "8671  @bumkeyyfel clowns? i'm not scared of clowns t...      0\n",
            "3441  @SachinKalbag Ah Sachin, why do you bring up u...      1\n",
            "9915  @annnalucz i'm not going :( kailan ba? may tix...      0\n",
            "6466                     @horan_lyra done, please me :(      0\n",
            "8630  I'm craving breakfast food so badly right now ...      0\n",
            "\n",
            "[8000 rows x 2 columns]\n",
            "                                                  tweet  label\n",
            "3688                             @btsmaqnae followed :)      1\n",
            "671               @4eyedmonk awesome :) I'll be waiting      1\n",
            "942   Thanks for connecting @garrowlscq Hope you're ...      1\n",
            "613    @cotterw @urihoresh we make it better though :-)      1\n",
            "3618  @monolifemusic @DJANORAK I'm getting there! It...      1\n",
            "...                                                 ...    ...\n",
            "5855  Snapchat me : LisaHerring19 #snapchat #kikme #...      0\n",
            "1401           Worth It - Fifth Harmony, don't judge :)      1\n",
            "6862                @taeserasera OMG! What happened? :(      0\n",
            "9644                       @fvvxkk that's a damn lie :(      0\n",
            "524        The long wait is over :)\\n#OTWOLGrandTrailer      1\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hENqcGGeBjx"
      },
      "source": [
        "####TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXgh5E4PeEdx"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(lowercase = False)\n",
        "tfidf_representation = tfidf_vectorizer.fit(train_df['tweet'])\n",
        "\n",
        "X_train = tfidf_vectorizer.transform(train_df['tweet'])\n",
        "X_test = tfidf_vectorizer.transform(test_df['tweet'])\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMMWmLnegV50"
      },
      "source": [
        "Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SleRBR3Gfcx2",
        "outputId": "6119685b-95c4-42a4-f90d-23446113a1ba"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaRmuGBPfqH8",
        "outputId": "0ddb97b4-bc37-47b0-853f-7c91b7e0957e"
      },
      "source": [
        "print('Train Score', logreg.score(X_train, y_train))\n",
        "print('Test Score', logreg.score(X_test, y_test))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score 0.897125\n",
            "Test Score 0.7635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAsQmZ9Q5J_"
      },
      "source": [
        "##Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/qTzLy6F6jkUtQrvy7 until November 17th"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj_ckmB_Q8JS"
      },
      "source": [
        "Investigate the effect of text normalization.\n",
        "\n",
        "- Search for a dataset for classification (or experiment with the same dataset from this lab)\n",
        "- Preprocess the text\n",
        "- Compare the vocabulary size with and without preprocessing\n",
        "- Get the numerical representation of the text\n",
        "- Train a model\n",
        "- Test your model \n",
        "- Compare the performance of your model with and without text normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__AIRTKuqbaQ"
      },
      "source": [
        "####Further reading\n",
        "\n",
        "- [TF-IDF/Term Frequency](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3)\n",
        "- [Bag of Words](https://www.mygreatlearning.com/blog/bag-of-words/)"
      ]
    }
  ]
}